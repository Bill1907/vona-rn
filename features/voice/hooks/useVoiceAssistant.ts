import { supabase } from "@/api/supabaseClient";
import { Audio } from "expo-av";
import { useCallback, useRef, useState } from "react";
import { Platform } from "react-native";
import { VoiceService } from "../services";
import type {
  VoiceAssistantAPI,
  VoiceAssistantState,
  VoiceMessage,
  VoiceSessionConfig,
} from "../types";
import { useWebrtc } from "./useWebrtc";

// React Native WebRTCë¥¼ ìœ„í•œ í´ë¦¬í•„
let InCallManager: any;
if (Platform.OS !== "web") {
  // React Native í™˜ê²½ì—ì„œëŠ” WebRTC í´ë¦¬í•„ì„ ì‚¬ìš©
  require("react-native-webrtc");
  const { mediaDevices } = require("react-native-webrtc");

  // InCallManager ê°€ì ¸ì˜¤ê¸° (ì˜¤ë””ì˜¤ ë¼ìš°íŒ… ì œì–´ìš©)
  try {
    InCallManager = require("react-native-webrtc").InCallManager;
  } catch (error) {
    console.warn("InCallManager not available:", error);
  }

  // Global navigatorì— mediaDevices ì¶”ê°€
  if (typeof global !== "undefined") {
    if (!global.navigator) {
      (global as any).navigator = {};
    }

    if (!global.navigator.mediaDevices) {
      (global.navigator as any).mediaDevices = mediaDevices;
    }
  }
}

export const useVoiceAssistant = (): VoiceAssistantAPI => {
  const [state, setState] = useState<VoiceAssistantState>({
    isConnected: false,
    isConnecting: false,
    isListening: false,
    isSpeaking: false,
    sessionId: null,
    error: null,
    isMicMuted: false,
    isSpeakerMuted: false,
  });

  const [messages, setMessages] = useState<VoiceMessage[]>([]);
  const dataChannelRef = useRef<RTCDataChannel | null>(null);
  const audioElementRef = useRef<HTMLAudioElement | null>(null);
  const ephemeralTokenRef = useRef<string | null>(null);
  const micStreamRef = useRef<MediaStream | null>(null);
  const webrtc = useWebrtc({
    onIceCandidate: (candidate) => {
      console.log("ICE candidate:", candidate);
    },
    onTrack: (event) => {
      console.log("Remote track received:", event);
      // ì›ê²© ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¬ìƒ
      if (event.streams[0]) {
        playRemoteAudio(event.streams[0]);
      }
    },
    onConnectionStateChange: (state) => {
      console.log("Connection state changed:", state);
      setState((prev) => ({
        ...prev,
        isConnected: state === "connected",
      }));
    },
  });

  // ì›¹ ê²€ìƒ‰ function calling ì²˜ë¦¬
  const handleWebSearch = useCallback(
    async (query: string, language: string = "ko", count: number = 5) => {
      try {
        const { data, error } = await supabase.functions.invoke("web-search", {
          body: {
            query,
            language,
            count,
          },
        });

        if (error) {
          console.error("Web search error:", error);
          return {
            error: "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            results: [],
          };
        }

        return {
          query: data.query,
          results: data.results || [],
          answer: data.answer,
          total_results: data.total_results || 0,
        };
      } catch (error) {
        console.error("Web search function error:", error);
        return {
          error: "ê²€ìƒ‰ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
          results: [],
        };
      }
    },
    []
  );

  /**
   * ì˜¤ë””ì˜¤ ëª¨ë“œë¥¼ ìŠ¤í”¼ì»¤í°ìœ¼ë¡œ ì„¤ì •
   */
  const setupSpeakerphone = useCallback(async () => {
    if (Platform.OS !== "web") {
      try {
        // Expo Audio ì„¸ì…˜ ì„¤ì • (ìŠ¤í”¼ì»¤í° ëª¨ë“œ)
        await Audio.setAudioModeAsync({
          allowsRecordingIOS: true,
          staysActiveInBackground: false,
          playsInSilentModeIOS: true,
          shouldDuckAndroid: false,
          playThroughEarpieceAndroid: false, // ìŠ¤í”¼ì»¤ë¡œ ì¬ìƒ
        });

        // InCallManagerë¡œ ìŠ¤í”¼ì»¤í° ê°•ì œ ì„¤ì •
        if (InCallManager) {
          InCallManager.setSpeakerphoneOn(true);
          InCallManager.setForceSpeakerphoneOn(true);
        }

        console.log("Speakerphone mode activated");
      } catch (error) {
        console.error("Failed to set speakerphone mode:", error);
      }
    }
  }, []);

  /**
   * ì˜¤ë””ì˜¤ ëª¨ë“œ ë³µì›
   */
  const restoreAudioMode = useCallback(async () => {
    if (Platform.OS !== "web") {
      try {
        // Expo Audio ì„¸ì…˜ ë³µì›
        await Audio.setAudioModeAsync({
          allowsRecordingIOS: false,
          staysActiveInBackground: false,
          playsInSilentModeIOS: false,
          shouldDuckAndroid: true,
          playThroughEarpieceAndroid: true, // ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µì›
        });

        // InCallManager ìŠ¤í”¼ì»¤í° í•´ì œ
        if (InCallManager) {
          InCallManager.setSpeakerphoneOn(false);
          InCallManager.setForceSpeakerphoneOn(false);
        }

        console.log("Audio mode restored");
      } catch (error) {
        console.error("Failed to restore audio mode:", error);
      }
    }
  }, []);

  /**
   * Voice Assistant ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤
   */
  const startVoiceSession = useCallback(
    async (config?: Partial<VoiceSessionConfig>) => {
      try {
        setState((prev) => ({ ...prev, error: null, isConnecting: true }));

        // ìŠ¤í”¼ì»¤í° ëª¨ë“œ ì„¤ì •
        await setupSpeakerphone();

        // 1. Supabase í•¨ìˆ˜ë¥¼ í†µí•´ ephemeral token ë°›ì•„ì˜¤ê¸°
        const { token, error: tokenError } =
          await VoiceService.createVoiceSession(config);

        if (tokenError || !token) {
          throw new Error(tokenError || "Failed to get ephemeral token");
        }

        ephemeralTokenRef.current = token.token;

        // 2. ë§ˆì´í¬ ê¶Œí•œ ë° ìŠ¤íŠ¸ë¦¼ íšë“
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: true,
        });
        if (!stream) {
          throw new Error("Failed to get microphone access");
        }

        // ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ì°¸ì¡° ì €ì¥ (ë®¤íŠ¸ ì œì–´ìš©)
        micStreamRef.current = stream;

        // 3. WebRTC PeerConnection ì´ˆê¸°í™”
        await webrtc.initializePeerConnection();

        // 4. ë°ì´í„° ì±„ë„ ìƒì„± (OpenAI ë©”ì‹œì§€ í†µì‹ ìš©)
        const dataChannel = webrtc.createDataChannel("oai-events");
        dataChannelRef.current = dataChannel;

        // ë°ì´í„° ì±„ë„ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì„¤ì •
        dataChannel.addEventListener("open", () => {
          console.log("Data channel opened");

          // ì„¸ì…˜ ì„¤ì • ë©”ì‹œì§€ ì „ì†¡
          const sessionUpdate = {
            type: "session.update",
            session: {
              instructions:
                config?.instructions || "You are a helpful assistant.",
              voice: config?.voice || "alloy",
              temperature: config?.temperature || 0.8,
              input_audio_transcription: {
                model: "whisper-1",
              },
              turn_detection: {
                type: "server_vad",
                threshold: 0.5,
                prefix_padding_ms: 300,
                silence_duration_ms: 200,
                create_response: true,
              },
            },
          };

          dataChannel.send(JSON.stringify(sessionUpdate));

          // ì´ˆê¸° ì‘ë‹µ ìƒì„±
          const initialResponse = {
            type: "response.create",
            response: {
              modalities: ["text", "audio"],
              instructions: "Greet the user and ask how you can help them.",
            },
          };

          dataChannel.send(JSON.stringify(initialResponse));
        });

        dataChannel.addEventListener("message", (event: MessageEvent) => {
          try {
            const message = JSON.parse(event.data);
            console.log("Received message:", message);
            handleOpenAIMessage(message);
          } catch (error) {
            console.error("Failed to parse data channel message:", error);
          }
        });

        dataChannel.addEventListener("error", (error: Event) => {
          console.error("Data channel error:", error);
        });

        // 5. ë¡œì»¬ ì˜¤ë””ì˜¤ íŠ¸ë™ ì¶”ê°€
        const audioTrack = stream.getAudioTracks()[0];
        await webrtc.addTrack(audioTrack, stream);

        // 6. Offer ìƒì„±
        await webrtc.createOffer();
        const offer = webrtc.getLocalDescription();

        if (!offer) {
          throw new Error("Failed to create offer");
        }

        // 7. OpenAI Realtime APIì— offer ì „ì†¡ (ephemeral token ì‚¬ìš©)
        await sendOfferToOpenAI(offer, ephemeralTokenRef.current);

        setState((prev) => ({
          ...prev,
          sessionId: token.session_id,
          isConnecting: false,
        }));
      } catch (error) {
        console.error("Failed to start voice session:", error);
        setState((prev) => ({
          ...prev,
          isConnecting: false,
          error:
            error instanceof Error
              ? error.message
              : "Failed to start voice session",
        }));
      }
    },
    [webrtc]
  );

  /**
   * Voice Assistant ì„¸ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤
   */
  const endVoiceSession = useCallback(async () => {
    try {
      // ì˜¤ë””ì˜¤ ëª¨ë“œ ë³µì›
      restoreAudioMode();

      // ë°ì´í„° ì±„ë„ ì •ë¦¬
      if (dataChannelRef.current) {
        dataChannelRef.current.close();
        dataChannelRef.current = null;
      }

      // ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
      if (micStreamRef.current) {
        micStreamRef.current.getTracks().forEach((track) => track.stop());
        micStreamRef.current = null;
      }

      // WebRTC ì—°ê²° ì •ë¦¬
      webrtc.closeConnection();

      setState({
        isConnected: false,
        isConnecting: false,
        isListening: false,
        isSpeaking: false,
        sessionId: null,
        error: null,
        isMicMuted: false,
        isSpeakerMuted: false,
      });
    } catch (error) {
      console.error("Failed to end voice session:", error);
      setState((prev) => ({
        ...prev,
        error: "Failed to end voice session",
      }));
    }
  }, [webrtc, restoreAudioMode]);

  /**
   * ìŒì„± ë“£ê¸° ì‹œì‘/ì¤‘ì§€
   */
  const toggleListening = useCallback(() => {
    if (!state.isConnected) return;

    setState((prev) => ({
      ...prev,
      isListening: !prev.isListening,
    }));
  }, [state.isConnected]);

  /**
   * ë§ˆì´í¬ ìŒì†Œê±°/í•´ì œ
   */
  const toggleMicMute = useCallback(() => {
    if (!micStreamRef.current) return;

    const audioTracks = micStreamRef.current.getAudioTracks();
    audioTracks.forEach((track) => {
      track.enabled = state.isMicMuted; // í˜„ì¬ ë®¤íŠ¸ ìƒíƒœì˜ ë°˜ëŒ€ë¡œ ì„¤ì •
    });

    setState((prev) => ({
      ...prev,
      isMicMuted: !prev.isMicMuted,
    }));
  }, [state.isMicMuted]);

  /**
   * ìŠ¤í”¼ì»¤ ìŒì†Œê±°/í•´ì œ
   */
  const toggleSpeakerMute = useCallback(() => {
    if (Platform.OS === "web" && audioElementRef.current) {
      audioElementRef.current.muted = !state.isSpeakerMuted;
    } else if (Platform.OS !== "web" && InCallManager) {
      // React Nativeì—ì„œëŠ” InCallManagerë¡œ ìŠ¤í”¼ì»¤ ì œì–´
      if (state.isSpeakerMuted) {
        InCallManager.setSpeakerphoneOn(true);
      } else {
        InCallManager.setSpeakerphoneOn(false);
      }
    }

    setState((prev) => ({
      ...prev,
      isSpeakerMuted: !prev.isSpeakerMuted,
    }));
  }, [state.isSpeakerMuted]);

  /**
   * ì „ì²´ ìŒì†Œê±°/í•´ì œ (ë§ˆì´í¬ + ìŠ¤í”¼ì»¤)
   */
  const toggleMute = useCallback(() => {
    const shouldMute = !state.isMicMuted || !state.isSpeakerMuted;

    // ë§ˆì´í¬ ì œì–´
    if (micStreamRef.current) {
      const audioTracks = micStreamRef.current.getAudioTracks();
      audioTracks.forEach((track) => {
        track.enabled = !shouldMute;
      });
    }

    // ìŠ¤í”¼ì»¤ ì œì–´
    if (Platform.OS === "web" && audioElementRef.current) {
      audioElementRef.current.muted = shouldMute;
    } else if (Platform.OS !== "web" && InCallManager) {
      InCallManager.setSpeakerphoneOn(!shouldMute);
    }

    setState((prev) => ({
      ...prev,
      isMicMuted: shouldMute,
      isSpeakerMuted: shouldMute,
    }));
  }, [state.isMicMuted, state.isSpeakerMuted]);

  /**
   * í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡
   */
  const sendTextMessage = useCallback(
    async (text: string) => {
      if (!state.isConnected || !dataChannelRef.current) return;

      const message: VoiceMessage = {
        id: Date.now().toString(),
        type: "user",
        content: text,
        timestamp: new Date(),
      };

      setMessages((prev) => [...prev, message]);

      // ë°ì´í„° ì±„ë„ì„ í†µí•´ í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡
      try {
        const createMessage = {
          type: "conversation.item.create",
          item: {
            type: "message",
            role: "user",
            content: [{ type: "input_text", text }],
          },
        };

        dataChannelRef.current.send(JSON.stringify(createMessage));

        // ì‘ë‹µ ìƒì„± ìš”ì²­
        const responseCreate = {
          type: "response.create",
          response: {
            modalities: ["text", "audio"],
          },
        };

        dataChannelRef.current.send(JSON.stringify(responseCreate));
      } catch (error) {
        console.error("Failed to send text message:", error);
      }
    },
    [state.isConnected]
  );

  // Helper functions
  const sendOfferToOpenAI = async (
    offer: RTCSessionDescriptionInit,
    ephemeralToken: string
  ) => {
    try {
      // OpenAI Realtime APIì˜ ì˜¬ë°”ë¥¸ ì—”ë“œí¬ì¸íŠ¸ì™€ ë°©ì‹ (ephemeral token ì‚¬ìš©)
      const baseUrl = "https://api.openai.com/v1/realtime";
      const model = "gpt-4o-realtime-preview";

      const response = await fetch(`${baseUrl}?model=${model}`, {
        method: "POST",
        headers: {
          Authorization: `Bearer ${ephemeralToken}`,
          "Content-Type": "application/sdp",
        },
        body: offer.sdp,
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(
          `Failed to send offer to OpenAI: ${response.status} ${errorText}`
        );
      }

      const answerSdp = await response.text();
      console.log("Received answer SDP:", answerSdp);

      // Answerë¥¼ ë¡œì»¬ peer connectionì— ì„¤ì •
      await webrtc.setRemoteDescription({
        type: "answer",
        sdp: answerSdp,
      });
    } catch (error) {
      console.error("Error in sendOfferToOpenAI:", error);
      throw error;
    }
  };

  const handleOpenAIMessage = async (message: any) => {
    switch (message.type) {
      case "session.created":
        console.log("Session created:", message.session);
        break;

      case "session.updated":
        console.log("Session updated:", message.session);
        break;

      case "input_audio_buffer.speech_started":
        setState((prev) => ({ ...prev, isListening: true }));
        break;

      case "input_audio_buffer.speech_stopped":
        setState((prev) => ({ ...prev, isListening: false }));
        break;

      case "conversation.item.input_audio_transcription.completed":
        if (message.transcript) {
          const userMessage: VoiceMessage = {
            id: Date.now().toString(),
            type: "user",
            content: message.transcript,
            timestamp: new Date(),
          };
          setMessages((prev) => [...prev, userMessage]);
        }
        break;

      case "response.audio_transcript.done":
        if (message.transcript) {
          const assistantMessage: VoiceMessage = {
            id: Date.now().toString(),
            type: "assistant",
            content: message.transcript,
            timestamp: new Date(),
          };
          setMessages((prev) => [...prev, assistantMessage]);
        }
        break;

      case "response.function_call_arguments.done":
        // Function callì´ ì™„ë£Œë˜ì—ˆì„ ë•Œ ì²˜ë¦¬
        console.log("ğŸ”§ Function call arguments completed:", message);
        if (message.name === "web_search" && dataChannelRef.current) {
          try {
            const args = JSON.parse(message.arguments);
            const { query, language = "ko", count = 5 } = args;

            console.log("ğŸ” Executing web search:", { query, language, count });
            setState((prev) => ({ ...prev, isSpeaking: true }));

            const searchResult = await handleWebSearch(query, language, count);

            // Function call ê²°ê³¼ë¥¼ OpenAIì— ì „ì†¡
            const functionResponse = {
              type: "conversation.item.create",
              item: {
                type: "function_call_output",
                call_id: message.call_id,
                output: JSON.stringify(searchResult),
              },
            };

            dataChannelRef.current.send(JSON.stringify(functionResponse));

            // ì‘ë‹µ ìƒì„± ìš”ì²­
            const responseCreate = {
              type: "response.create",
              response: {
                modalities: ["text", "audio"],
              },
            };

            dataChannelRef.current.send(JSON.stringify(responseCreate));

            console.log(
              "ğŸ” Web search completed and sent to OpenAI:",
              searchResult
            );
          } catch (error) {
            console.error("Function call execution error:", error);

            // ì—ëŸ¬ ì‘ë‹µ ì „ì†¡
            if (dataChannelRef.current) {
              const errorResponse = {
                type: "conversation.item.create",
                item: {
                  type: "function_call_output",
                  call_id: message.call_id,
                  output: JSON.stringify({
                    error: "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                  }),
                },
              };

              dataChannelRef.current.send(JSON.stringify(errorResponse));
            }
          }
        }
        break;

      case "response.done":
        setState((prev) => ({ ...prev, isSpeaking: false }));
        break;

      case "error":
        console.error("OpenAI error:", message);
        setState((prev) => ({
          ...prev,
          error: message.error?.message || "OpenAI API error",
        }));
        break;

      default:
        console.log("Unhandled message type:", message.type);
    }
  };

  const playRemoteAudio = (stream: MediaStream) => {
    if (Platform.OS === "web") {
      if (!audioElementRef.current) {
        audioElementRef.current = new HTMLAudioElement();
      }

      if (audioElementRef.current) {
        audioElementRef.current.srcObject = stream;
        audioElementRef.current.play().catch(console.error);

        setState((prev) => ({ ...prev, isSpeaking: true }));

        audioElementRef.current.onended = () => {
          setState((prev) => ({ ...prev, isSpeaking: false }));
        };
      }
    } else {
      // React Nativeì—ì„œëŠ” WebRTCê°€ ìë™ìœ¼ë¡œ ì˜¤ë””ì˜¤ë¥¼ ì²˜ë¦¬
      // ìŠ¤í”¼ì»¤í° ëª¨ë“œê°€ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ìŠ¤í”¼ì»¤ë¡œ ì¬ìƒë¨
      setState((prev) => ({ ...prev, isSpeaking: true }));

      // ìŠ¤íŠ¸ë¦¼ì´ ì¢…ë£Œë˜ë©´ speaking ìƒíƒœ í•´ì œ
      const tracks = stream.getAudioTracks();
      if (tracks.length > 0) {
        tracks[0].onended = () => {
          setState((prev) => ({ ...prev, isSpeaking: false }));
        };
      }
    }
  };

  return {
    state,
    messages,
    startVoiceSession,
    endVoiceSession,
    toggleListening,
    toggleMicMute,
    toggleSpeakerMute,
    toggleMute,
    sendTextMessage,
  };
};
