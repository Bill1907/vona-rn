import { supabase } from "@/api/supabaseClient";
import {
  RealtimeAgent,
  RealtimeSession,
  RealtimeSessionConnectOptions,
  TransportEvent,
} from "@openai/agents-realtime";
import { useCallback, useRef, useState } from "react";
import { VoiceService } from "../services";
import type {
  VoiceAssistantAPI,
  VoiceAssistantState,
  VoiceMessage,
  VoiceSessionConfig,
} from "../types";

export const useVoiceAssistant = (): VoiceAssistantAPI => {
  const [state, setState] = useState<VoiceAssistantState>({
    isConnected: false,
    isConnecting: false,
    isListening: false,
    isSpeaking: false,
    sessionId: null,
    error: null,
    isMicMuted: false,
    isSpeakerMuted: false,
  });

  const [messages, setMessages] = useState<VoiceMessage[]>([]);
  const sessionRef = useRef<RealtimeSession | null>(null);
  const agentRef = useRef<RealtimeAgent | null>(null);

  // ì›¹ ê²€ìƒ‰ function calling ì²˜ë¦¬ (í˜„ì¬ ì›¹ SDKì—ì„œëŠ” ì§€ì›ë˜ì§€ ì•ŠìŒ)
  const handleWebSearch = useCallback(
    async (query: string, language: string = "ko", count: number = 5) => {
      try {
        const { data, error } = await supabase.functions.invoke("web-search", {
          body: {
            query,
            language,
            count,
          },
        });

        if (error) {
          console.error("Web search error:", error);
          return {
            error: "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            results: [],
          };
        }

        return {
          query: data.query,
          results: data.results || [],
          answer: data.answer,
          total_results: data.total_results || 0,
        };
      } catch (error) {
        console.error("Web search function error:", error);
        return {
          error: "ê²€ìƒ‰ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
          results: [],
        };
      }
    },
    []
  );

  /**
   * Voice Assistant ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤
   */
  const startVoiceSession = useCallback(
    async (config?: Partial<VoiceSessionConfig>) => {
      try {
        setState((prev) => ({ ...prev, error: null, isConnecting: true }));

        // 1. Supabase í•¨ìˆ˜ë¥¼ í†µí•´ ephemeral token ë°›ì•„ì˜¤ê¸°
        const { token, error: tokenError } =
          await VoiceService.createVoiceSession(config);

        if (tokenError || !token) {
          throw new Error(tokenError || "Failed to get ephemeral token");
        }

        // 2. RealtimeAgent ìƒì„±
        const agent = new RealtimeAgent({
          name: "Voice Assistant",
          instructions: config?.instructions || "You are a helpful assistant.",
        });
        agentRef.current = agent;

        // 3. RealtimeSession ìƒì„± - ì˜¬ë°”ë¥¸ ì˜µì…˜ êµ¬ì¡° ì‚¬ìš©
        const session = new RealtimeSession(agent, {
          apiKey: token.token,
          transport: "webrtc", // WebRTC ì‚¬ìš© (ë¸Œë¼ìš°ì € í™˜ê²½)
          model: config?.model || "gpt-4o-realtime-preview",
          config: {
            voice: config?.voice || "alloy",
            inputAudioTranscription: { model: "whisper-1" },
            turnDetection: {
              type: "server_vad",
              threshold: 0.5,
              prefix_padding_ms: 300,
              silence_duration_ms: 200,
              create_response: true,
            },
          },
        });

        // 4. ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì„¤ì • - SDKì˜ ì‹¤ì œ ì´ë²¤íŠ¸ ì‚¬ìš©
        session.on("agent_start", () => {
          console.log("ğŸ™ï¸ Agent started speaking");
          setState((prev) => ({ ...prev, isSpeaking: true }));
        });

        session.on("agent_end", () => {
          console.log("ğŸ™ï¸ Agent stopped speaking");
          setState((prev) => ({ ...prev, isSpeaking: false }));
        });

        session.on("audio_start", () => {
          console.log("ğŸ”Š Audio output started");
          setState((prev) => ({ ...prev, isSpeaking: true }));
        });

        session.on("audio_stopped", () => {
          console.log("ğŸ”Š Audio output stopped");
          setState((prev) => ({ ...prev, isSpeaking: false }));
        });

        session.on("history_updated", (history) => {
          // ëŒ€í™” ê¸°ë¡ì´ ì—…ë°ì´íŠ¸ë  ë•Œ ë©”ì‹œì§€ ë™ê¸°í™”
          const newMessages: VoiceMessage[] = [];

          history.forEach((item: any) => {
            if (item.type === "message") {
              if (
                item.role === "user" &&
                item.content?.[0]?.type === "input_text"
              ) {
                newMessages.push({
                  id: item.id || Date.now().toString(),
                  type: "user",
                  content: item.content[0].text,
                  timestamp: new Date(),
                });
              } else if (
                item.role === "assistant" &&
                item.content?.[0]?.type === "text"
              ) {
                newMessages.push({
                  id: item.id || Date.now().toString(),
                  type: "assistant",
                  content: item.content[0].text,
                  timestamp: new Date(),
                });
              }
            }
          });

          setMessages(newMessages);
        });

        session.on("error", (error: any) => {
          console.error("Session error:", error);
          setState((prev) => ({
            ...prev,
            error: error.error?.message || "Session error occurred",
          }));
        });

        // transport_eventëŠ” ë‚®ì€ ìˆ˜ì¤€ì˜ ì´ë²¤íŠ¸ë¥¼ ìœ„í•´ ë³´ì¡°ì ìœ¼ë¡œ ì‚¬ìš©
        session.on("transport_event", (event: TransportEvent) => {
          switch (event.type) {
            case "session.created":
              console.log("Session created");
              setState((prev) => ({
                ...prev,
                isConnected: true,
                isConnecting: false,
                sessionId: token.session_id,
              }));
              break;

            case "input_audio_buffer.speech_started":
              setState((prev) => ({ ...prev, isListening: true }));
              break;

            case "input_audio_buffer.speech_stopped":
              setState((prev) => ({ ...prev, isListening: false }));
              break;

            case "conversation.item.input_audio_transcription.completed":
              // ì´ë¯¸ history_updatedì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ ì¤‘ë³µ ë°©ì§€
              console.log("Audio transcription completed:", event);
              break;

            // TODO: Function calling ì§€ì›ì´ ì¶”ê°€ë˜ë©´ ì—¬ê¸°ì„œ ì²˜ë¦¬
            case "response.function_call_arguments.done":
              console.log(
                "Function call detected (not yet supported in web):",
                event
              );
              // ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì€ í˜„ì¬ React Native ë²„ì „ì—ì„œë§Œ ì§€ì›
              break;

            default:
              console.log("Transport event:", event.type);
          }
        });

        // 5. ì—°ê²° ì‹œì‘
        const connectOptions: RealtimeSessionConnectOptions = {
          apiKey: token.token,
          model: config?.model || "gpt-4o-realtime-preview",
        };

        await session.connect(connectOptions);

        sessionRef.current = session;

        // 6. ì›¹ í™˜ê²½ ì˜¤ë””ì˜¤ ì„¤ì • ë° ê¶Œí•œ ì²˜ë¦¬
        if (typeof window !== "undefined") {
          // ë¸Œë¼ìš°ì € ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ í™œì„±í™” (ìë™ì¬ìƒ ì •ì±… ëŒ€ì‘)
          try {
            // AudioContext ìƒì„± ë° resume (ì‚¬ìš©ì ì œìŠ¤ì²˜ê°€ ìˆëŠ” ìƒíƒœì—ì„œ)
            const audioContext = new (window.AudioContext ||
              (window as any).webkitAudioContext)();
            if (audioContext.state === "suspended") {
              audioContext
                .resume()
                .then(() => {
                  console.log("AudioContext resumed successfully");
                })
                .catch((error) => {
                  console.warn("Failed to resume AudioContext:", error);
                });
            }

            // Transportì˜ ì˜¤ë””ì˜¤ ìš”ì†Œ ì„¤ì •
            setTimeout(() => {
              const transport = session.transport;
              if (transport && "audioElement" in transport) {
                const audioElement = (transport as any).audioElement;
                if (audioElement) {
                  // ì˜¤ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ ê¸°ë³¸ ì„¤ì •
                  audioElement.autoplay = true;
                  audioElement.muted = false;
                  audioElement.volume = 1.0;
                  audioElement.setAttribute("playsinline", "true");

                  // ì˜¤ë””ì˜¤ ì¬ìƒ ì‹œë„ (ìë™ì¬ìƒ ì •ì±… í…ŒìŠ¤íŠ¸)
                  audioElement.play().catch((error: any) => {
                    if (error.name === "NotAllowedError") {
                      console.warn(
                        "Audio autoplay blocked by browser policy. User interaction required."
                      );
                    } else {
                      console.error("Audio play error:", error);
                    }
                  });

                  console.log("Audio element configured:", audioElement);
                }
              }
            }, 1000);
          } catch (error) {
            console.error("Audio setup error:", error);
          }
        }
      } catch (error) {
        console.error("Failed to start voice session:", error);
        setState((prev) => ({
          ...prev,
          isConnecting: false,
          error:
            error instanceof Error
              ? error.message
              : "Failed to start voice session",
        }));
      }
    },
    []
  );

  /**
   * Voice Assistant ì„¸ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤
   */
  const endVoiceSession = useCallback(async () => {
    try {
      if (sessionRef.current) {
        sessionRef.current.close(); // disconnect ëŒ€ì‹  close ì‚¬ìš©
        sessionRef.current = null;
      }

      agentRef.current = null;

      setState({
        isConnected: false,
        isConnecting: false,
        isListening: false,
        isSpeaking: false,
        sessionId: null,
        error: null,
        isMicMuted: false,
        isSpeakerMuted: false,
      });
    } catch (error) {
      console.error("Failed to end voice session:", error);
      setState((prev) => ({
        ...prev,
        error: "Failed to end voice session",
      }));
    }
  }, []);

  /**
   * ìŒì„± ë“£ê¸° ì‹œì‘/ì¤‘ì§€ (ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨)
   */
  const toggleListening = useCallback(() => {
    // OpenAI SDKëŠ” ìë™ìœ¼ë¡œ VADë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ ë³„ë„ êµ¬í˜„ ë¶ˆí•„ìš”
    console.log("Voice activity detection is handled automatically");
  }, []);

  /**
   * ë§ˆì´í¬ ìŒì†Œê±°/í•´ì œ
   */
  const toggleMicMute = useCallback(() => {
    if (!sessionRef.current) return;

    const newMuteState = !state.isMicMuted;

    // Sessionì˜ mute ë©”ì„œë“œ ì‚¬ìš©
    sessionRef.current.mute(newMuteState);

    setState((prev) => ({
      ...prev,
      isMicMuted: newMuteState,
    }));
  }, [state.isMicMuted]);

  /**
   * ìŠ¤í”¼ì»¤ ìŒì†Œê±°/í•´ì œ
   */
  const toggleSpeakerMute = useCallback(() => {
    if (!sessionRef.current) return;

    const newMuteState = !state.isSpeakerMuted;

    // transport layerì˜ ì˜¤ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ë¥¼ ì§ì ‘ ì œì–´
    const transport = sessionRef.current.transport;
    if (transport && "audioElement" in transport) {
      const audioElement = (transport as any).audioElement;
      if (audioElement) {
        audioElement.muted = newMuteState;

        // ìŒì†Œê±° í•´ì œ ì‹œ ì˜¤ë””ì˜¤ ì¬ìƒ í™œì„±í™” ì‹œë„
        if (!newMuteState) {
          audioElement.play().catch((error: any) => {
            console.warn("Audio play failed during unmute:", error);
          });
        }
      }
    }

    setState((prev) => ({
      ...prev,
      isSpeakerMuted: newMuteState,
    }));
  }, [state.isSpeakerMuted]);

  /**
   * ì „ì²´ ìŒì†Œê±°/í•´ì œ
   */
  const toggleMute = useCallback(() => {
    const shouldMute = !state.isMicMuted || !state.isSpeakerMuted;

    if (sessionRef.current) {
      // ë§ˆì´í¬ ë®¤íŠ¸
      sessionRef.current.mute(shouldMute);

      // ìŠ¤í”¼ì»¤ ë®¤íŠ¸
      const transport = sessionRef.current.transport;
      if (transport && "audioElement" in transport) {
        const audioElement = (transport as any).audioElement;
        if (audioElement) {
          audioElement.muted = shouldMute;

          // ìŒì†Œê±° í•´ì œ ì‹œ ì˜¤ë””ì˜¤ ì¬ìƒ í™œì„±í™” ì‹œë„
          if (!shouldMute) {
            audioElement.play().catch((error: any) => {
              console.warn("Audio play failed during unmute:", error);
            });
          }
        }
      }
    }

    setState((prev) => ({
      ...prev,
      isMicMuted: shouldMute,
      isSpeakerMuted: shouldMute,
    }));
  }, [state.isMicMuted, state.isSpeakerMuted]);

  /**
   * í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡
   */
  const sendTextMessage = useCallback(
    async (text: string) => {
      if (!state.isConnected || !sessionRef.current) return;

      const message: VoiceMessage = {
        id: Date.now().toString(),
        type: "user",
        content: text,
        timestamp: new Date(),
      };

      setMessages((prev) => [...prev, message]);

      try {
        // sendMessageëŠ” string ë˜ëŠ” íŠ¹ì • í˜•ì‹ì˜ ê°ì²´ë¥¼ ë°›ìŒ
        sessionRef.current.sendMessage({
          type: "message",
          role: "user",
          content: [{ type: "input_text", text }],
        });
      } catch (error) {
        console.error("Failed to send text message:", error);
      }
    },
    [state.isConnected]
  );

  return {
    state,
    messages,
    startVoiceSession,
    endVoiceSession,
    toggleListening,
    toggleMicMute,
    toggleSpeakerMute,
    toggleMute,
    sendTextMessage,
  };
};
